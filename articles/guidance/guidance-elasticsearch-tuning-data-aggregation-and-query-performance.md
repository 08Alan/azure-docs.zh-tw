<properties
   pageTitle="利用 Azure 上的 Elasticsearch 調整資料彙總與查詢的效能 | Microsoft Azure"
   description="摘要說明 Elasticsearch 查詢和搜尋最佳化的考慮事項。"
   services=""
   documentationCenter="na"
   authors="mabsimms"
   manager="marksou"
   editor=""
   tags=""/>

<tags
   ms.service="guidance"
   ms.devlang="na"
   ms.topic="article"
   ms.tgt_pltfrm="na"
   ms.workload="na"
   ms.date="02/29/2016"
   ms.author="masimms"/>
   
# 利用 Azure 上的 Elasticsearch 調整資料彙總與查詢的效能


本文是[系列文章的其中一篇](guidance-elasticsearch.md)。

使用 Elasticsearch 的主要原因是要支援經由資料的搜尋。使用者應該能夠快速找到他們要尋找的資訊。此外，系統必須能夠讓使用者針對資料問問題、尋找相互關係、得出結論，以利商務決策；就是這個過程讓資料變成資訊。

本文摘要說明在您決定讓系統查詢和搜尋獲得最佳效能的最佳方式時，可以考慮的選項。

所有效能建議大多取決於適用於您情況的案例、您編製索引的資料量、以及應用程式和使用者查詢您的資料的速率。您應該使用自己的資料和工作負載來評估特定案例的優點，仔細測試任何組態或編製索引結構變更的結果。為了這個目的，這份文件也將說明使用不同組態來實作一個特定案例時所執行的一些基準。您可以直接採用我們的方法來評估您系統的效能。這些測試的詳細資料將在[附錄](#appendix-the-query-and-aggregation-performance-test)中說明。

## 索引和查詢的效能考量

本章節描述在設計需要支援快速查詢和搜尋的索引時，您應考慮的一些共通因素。

### 在索引中儲存多個類型

Elasticsearch 索引可以包含多個類型。最好避免使用這種方法，而是為每個類型建立個別索引。請考慮下列幾點：

- 不同類型可能指定不同分析器，如果在索引層級執行查詢而不是在類型層級執行，則不一定了解 Elasticsearch 應該使用哪個分析器。詳細資訊請參閱[避免類型錯誤](https://www.elastic.co/guide/en/elasticsearch/guide/current/mapping.html#_avoiding_type_gotchas)。

- 包含多個類型的索引分區可能會比包含單一類型的索引更大。分區愈大，Elasticsearch 執行查詢時需要進行更多工作來篩選資料。

- 如果不同類型的資料量相差很多，一個類型的資訊可能會分散到許多分區而變得稀疏，以致於降低擷取這項資料的搜尋的效率。

    ![](./media/guidance-elasticsearch/query-performance1.png)

    ***圖 1.類型共用索引的效果***

    圖 1 說明這種案例。在圖的上半部，類型 A 和類型 B 的文件中共用相同的索引。類型 A 的文件比類型 B 多很多。針對類型 A 的搜尋會查詢所有的四個分區。圖的下半部顯示為每個類型建立個別索引的效果。在此情況下，搜尋類型 A 只需要存取兩個分區。

- 小分區可能比大分區更平均分佈，讓 Elasticsearch 更容易在節點之間分散負載。

- 不同類型可能會有不同的保留期間。封存與現用資料共用分區的舊資料可能有困難。


不過在某些情況下，不同類型共用索引可以很有效率，前提是：

- 會定期在相同索引中進行跨類型的搜尋。

- 每個類型只有少量的文件；在此情況下，個別為每個類型維護一個分區的負擔可能會相當大。


### 最佳化索引類型

Elasticsearch 索引包含用來填入它的原始 JSON 文件。這項資訊會放在每個索引項目的 [*\_source*](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-source-field.html#mapping-source-field) 欄位。這項資料不可搜尋，但預設會由 *get* 和 *search* 要求傳回。不過，此欄位會產生額外負荷並佔用儲存空間，讓分區更大，增加執行的 I/O 量。您可以停用各類型的 *\_source* 欄位：

```http
PUT my_index
{
  "mappings": {
    "my_type": {
      "_source": {
        "enabled": false
      }
    }
  }
}
```
停用此欄位也會失去執行下列作業的能力：

- 使用 *update* API 更新索引中的資料。

- 執行可傳回強調顯示資料的搜尋。

- 直接從一個 Elasticsearch 索引重新編製另一個索引。

- 變更對應或分析設定。

- 藉由檢視原始文件來偵錯查詢。


### 為資料重新編製索引

索引可用的分區數目最終會決定索引的容量。您可以初步 (明智的) 推測將需要多少分區，但您永遠應該優先考慮您的文件重新編製索引策略。在許多情況下，隨著資料增加而重新編製索引無可避免；為了要得到最佳搜尋結果，您可能不希望一開始就分配大量分區給索引，而是希望隨著資料量擴大而配置新的分區。在其他情況下，您可能需要為更特定的情形重新編製索引 － 單純是您對資料量成長的估計不正確。

> [AZURE.NOTE] 快速過時的資料不需要重新編製索引。在此情況下，應用程式可能會為每一段時間建立新的索引。效能記錄檔、儲存在每天最新索引中的稽核資料都是例子。

<!-- -->

有效率地重新編製索引牽涉到從舊的資料建立新的索引，然後再移除舊索引。如果索引很大，此程序很花費時間，而且您需要確保資料在這段期間仍可搜尋。基於這個理由，您應該建立[每個索引的別名](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html)，且查詢應該透過這些別名擷取資料。重新編製索引時，保留指向舊索引的別名，然後在重新編製索引完成後，將別名轉向參考新索引。這個方法也可用於存取時間型資料 (每天會建立新的索引)；若要存取目前的資料，使用建立新索引時變換的別名。

### 管理對應

Elasticsearch 使用對應來決定如何解譯文件中每個欄位出現的資料。每個類型都有它自己的對應，有效地定義該類型的結構描述。Elasticsearch 使用此資訊來產生一個類型的文件中，每個欄位的反向索引。在任何文件中，每個欄位都有一個資料類型 (例如 *string*、*date* 或 *long*) 和一個值。第一次建立索引後，您可以指定索引的對應，或是讓 Elasticsearch 能夠在類型中加入文件後加以推斷。不過，請考慮下列幾點：

- 動態產生的對應可能會導致錯誤，取決於文件加入索引時的欄位解譯方式。例如，文件 1 包含保存數字的欄位 A，造成 Elasticsearch 加入指定這個欄位是 *long* 的對應。如果後續加入的文件中欄位 A 包含非數值資料，對應將失敗。在此案例中，加入第一個文件時，欄位 A 可能已被解譯為字串。在建立索引時指定此對應，可協助您避免這類問題。

- 設計您的文件，以避免產生過多的對應，因為這可能會在執行搜尋時大幅增加負擔，消耗大量記憶體，並且造成查詢找不到尋找資料。對共用相同類型的文件中的欄位採用一致的命名慣例。例如，請勿在不同的文件中使用像「first\_name」、「FirstName」、「名字」等欄位名稱，請在每份文件中使用相同的欄位名稱。此外，請勿嘗試使用值做為索引鍵 (這在資料欄系列的資料庫中是常見的方法，但會造成效率不彰和 Elasticsearch 失敗)。 如需詳細資訊，請參閱[對應遽增](https://www.elastic.co/blog/found-crash-elasticsearch#mapping-explosion)。

- 在適當的地方使用 *not\_analyzed* 避免 Token 化。例如，如果文件包含名為 *data* 的字串欄位，用於保存值 "ABC-DEF"，則您可能會嘗試搜尋所有符合這個值的文件，如下所示：

  ```http
  GET /myindex/mydata/_search
  {
    "query" : {
      "filtered" : {
        "filter" : {
          "term" : {
            "data" : "ABC-DEF"
          }
        }
      }
    }
  }
  ```

    However, this search will fail to return the expected results due to the way in which the string ABC-DEF is tokenized when it is indexed; it will be effectively split into two tokens, ABC and DEF, by the hyphen. This feature is designed to support full text searching, but if you want the string to be interpreted as a single atomic item you should disable tokenization when the document is added to the index. You can use a mapping such as this:

  ```http
  PUT /myindex
  {
    "mappings" : {
      "mydata" : {
        "properties" : {
          "data" : {
            "type" : "string",
            "index" : "not_analyzed"
          }
        }
      }
    }
  }
  ```

  如需詳細資訊，請參閱[尋找實際值](https://www.elastic.co/guide/en/elasticsearch/guide/current/_finding_exact_values.html#_term_filter_with_text)。


### 使用文件值

許多查詢與彙總需要搜尋作業納入資料排序。排序必須能夠將一或多個字詞對應到一份文件清單。為了協助此程序，Elasticsearch 可以將用來做為排序鍵的欄位所有值載入記憶體。這項資訊稱為 *fielddata*。其目的是快取在記憶體中的 fielddata 會產生較少的 I/O，而且可能會比重複地從磁碟讀取相同的資料更快。不過，如果欄位有高基數，則將 fielddata 儲存在記憶體中會耗用大量的堆積空間，可能影響執行其他並行作業的能力，或甚至耗盡儲存空間導致 Elasticsearch 失敗。

ElasticSearch 也支援*文件值*，做為另一種方法。文件值類似記憶體中的一個 fielddata 項目，但是儲存在磁碟上，且是在資料儲存在索引時建立 (fielddata 是在查詢執行時動態建立)。 文件值不會耗用堆積空間，因此很適合用在會跨欄位排序或彙總資料的查詢 (這些欄位可能包含非常大量的唯一值)。此外，堆積的壓力降低，有助於抵銷從磁碟擷取資料和從記憶體讀取的效能差異；記憶體回收可能較少發生，其他利用到記憶體的並行作業就比較不會受影響。

您可以使用 *doc\_values* 屬性，個別啟用或停用索引各個屬性的文件值，如下列範例所示：

```http
PUT /myindex
{
  "mappings" : {
    "mydata" : {
      "properties" : {
        "data" : {
          ...
          "doc_values": true
        }
      }
    }
  }
}
```
> [AZURE.NOTE] Elasticsearch 2.0.0 及之後的版本預設會啟用文件值。

使用文件值的實際影響很可能與您自己的資料和查詢案例極度相關，因此請準備好執行效能測試以確立其實用性。您也應該注意，文件值不適用於分析的字串欄位。如需詳細資訊，請參閱[文件值](https://www.elastic.co/guide/en/elasticsearch/guide/current/doc-values.html#doc-values)。

### 使用複本減少查詢競爭

大幅提升查詢效能常用的策略是建立每個索引的許多複本。藉著從複本提取資料，可以滿足資料擷取作業。不過，此策略可能嚴重影響資料擷取作業的效能，所以在牽涉到混合工作負載的情況下需要小心使用。此外，只有在複本會分散到各節點，且不會與屬於相同索引的主要分區競用資源時，這項策略才能發揮優點。請記住，可以增加或減少索引的複本數目，以利動態索引。

### 使用分區要求快取

Elasticsearch 可以快取記憶體中每個分區上的查詢所要求的本機資料。如此可以更快速執行擷取相同資料的搜尋；也可以從記憶體而非從磁碟儲存體讀取資料。以這種方式快取資料可因此改善某些搜尋作業的效能，但是相對地減少可用於同時執行之其他工作的記憶體。另外還有來自快取的資料已過期的風險。唯有當分區重新整理且資料已變更時，快取中的資料才會失效；重新整理的頻率由索引的 *refresh\_interval* 值設定。

索引的要求快取預設為停用，但您可以啟用它，如下所示：

```http
PUT /myindex/_settings
{
  "index.requests.cache.enable": true
}
```

分區要求快取是最適合用於相對靜態的資訊，例如歷史記錄或記錄資料。

### 使用用戶端節點

第一個收到要求的節點會處理所有查詢。這個節點會進一步將要求傳送至所有包含被查詢索引之分區的其他節點，並接著累積結果以便傳回回應。如果查詢牽涉到彙總資料或執行複雜的計算，初始節點會負責執行適當的處理。如果您的系統必須支援相對少量的複雜查詢，請考慮建立用戶端節點的集區，以減緩資料節點的負載。反之，如果您的系統必須處理大量的簡單查詢，可直接將這些要求提交至資料節點，然後使用負載平衡器平均分散要求。

### 調整查詢

下列幾點摘要說明使 Elasticsearch 查詢效能達到最高的秘訣：

- 盡可能避免包含萬用字元的查詢。

- 如果相同的欄位可能會進行全文檢索搜尋和完全符合，請考慮將欄位的資料儲存為已分析和未分析形式。針對已分析欄位執行全文檢索搜尋，針對未分析欄位則執行完全符合。

- 只傳回必要的資料。如果您有大型文件，但應用程式只需要保留在欄位子集中的資訊，那麼便請從查詢傳回這個子集，而不要傳回整個文件。此策略可以減少叢集的網路頻寬需求。

- 可能的話，搜尋資料時使用篩選器而不是查詢。篩選器只會判斷文件是否符合指定的準則，而查詢則還會計算文件的相符程度 (計分)。就內部而言，篩選器所產生的值會儲存為點陣圖，並且表示每份文件相符/不相符，且可以由 Elasticsearch 快取。如果後續發生相同的篩選條件，可以從快取擷取點陣圖，並用來快速擷取相符的文件。如需詳細資訊，請參閱[內部篩選作業](https://www.elastic.co/guide/en/elasticsearch/guide/current/_finding_exact_values.html#_internal_filter_operation)。

- 使用 *bool* 篩選器執行靜態比較，並且針對動態計算的篩選器只使用 *and*、*or* 和 *not* 篩選器，動態計旳篩選器例如牽涉到指令碼的篩選器，或是 *geo-** 篩選器。

- 如果查詢結合 *bool* 篩選器和 *and*、*or* 或 *ot* 與 *geo-** 篩選器，請將 *and*/*or*/*not geo-** 篩選器放在最後，讓它們儘可能在最小的資料集上運作。

    同樣地，使用 *post\_filter* 執行耗費資源的篩選作業。這些篩選器將會最後執行。

- 使用彙總，而不是 Facet。請避免計算經過分析的彙總，或是有許多可能值的彙總。

    > **注意**：Elasticsearch 2.0.0 版中已移除 Facet。

- 使用 *cardinality*彙總，優先於 *value\_count* 彙總，除非您的應用程式需要有確切的相符項目計數。確切的計數可能很快就會過時，而許多應用程式只需要合理的近似值。

- 避免指令碼。在查詢和篩選器中的指令碼可能會耗費資源，並且不會快取結果。長時間執行的指令碼可能無限期地使用搜尋執行緒，讓後續的要求排入佇列。如果佇列已滿，將拒絕進一步的要求。

## 測試和分析彙總與搜尋效能

本節說明針對不同叢集和索引設定執行一系列測試的結果。執行了兩種類型的測試，如下所示：

- ***擷取和查詢*測試**。這個測試從空的索引開始，接著在測試中藉由執行大量插入作業填入索引 (每個作業加入 1000 個文件)。於此同時，一些設計用來搜尋前 15 分鐘期間所加入文件並產生彙總的查詢，重複間隔為 5 秒。這項測試通常可以執行 24 小時，以便重現具挑戰性的工作負載的效果，在此工作負載中包含以接近即時查詢的大規模資料擷取。

- ***僅查詢*測試**。這項測試類似於*擷取和查詢*測試，只除了省略擷取部分，以及每個節點上的索引會預先填入 1 億份文件。會執行修改過的查詢集。限制在過去 15 分鐘所加入文件的時間項目已移除，因為資料現在是靜態的資料。測試執行了 90 分鐘；建立效能模式所需的時間較少，因為資料量固定。

---

索引中的每個文件有相同的結構描述。下表摘要說明結構描述中的欄位：

名稱 | 類型 | 注意事項 |
  ----------------------------- | ------------ | -------------------------------------------------------- |
 組織 | String | 測試會產生 200 個唯一組織。 |
 CustomField1 - CustomField5 |String |共五個字串欄位，設為空字串。|
 DateTimeRecievedUtc |Timestamp |加入文件的日期和時間。|
 Host |String |此欄位設為空字串。|
 HttpMethod |String |此欄位設為下列值之一："POST"、"GET"、"PUT"。|
 HttpReferrer |String |此欄位設為空字串。|
 HttpRequest |String |此欄位會填入長度為 10 到 200 個字元的隨機文字。|
 HttpUserAgent |String |此欄位設為空字串。|
 HttpVersion |String |此欄位設為空字串。|
 OrganizationName |String |這個欄位設為和 Organization 欄位相同的值。|
 SourceIp |IP |此欄位包含 IP 位址，指出資料的「來源」。 |
 SourceIpAreaCode |Long |此欄位設為 0。|
 SourceIpAsnNr |String |此欄位設為 AS#####。|
 SourceIpBase10 |Long |此欄位設為 500。|
 SourceIpCountryCode |String |此欄位為 2 個字元的國碼。 |
 SourceIpCity |String |此欄位為字串，指明國家/地區的城市。 |
 SourceIpLatitude |兩倍 |此欄位包含隨機值。|
 SourceIpLongitude |兩倍 |此欄位包含隨機值。|
 SourceIpMetroCode |Long |此欄位設為 0。|
 SourceIpPostalCode |String |此欄位設為空字串。|
 SourceLatLong |Geo point |此欄位設為隨機地區點。|
 SourcePort |String |此欄位會填入一個隨機數字的字串表示法。|
 TargetIp |IP |會填入範圍從 0.0.100.100 到 255.9.100.100 的隨機 IP 位址。|
 SourcedFrom |String |此欄位設為 "MonitoringCollector" 字串。|
 TargetPort |String |此欄位會填入一個隨機數字的字串表示法。|
 Rating |String |此欄位會填入 20 個隨機選取的不同字串值的其中一個。|
 UseHumanReadableDateTimes |Boolean |此欄位設為 false。|
 
測試的每次反覆都以批次執行下列查詢。斜體名稱用來在這份文件的其餘部分指稱這些查詢。請注意，*僅查詢*測試中省略了時間準則 (在過去 15 分鐘內加入的文件)：

- 過去 15 分鐘內輸入多少個有 *Rating* 值的文件 (*依評等的計數*)?

- 過去 15 分鐘內，每 5 分鐘的間隔中加入多少個文件 (*不同時間的計數*)?

- 過去 15 分鐘內，每個國家/地區的每個 *Rating* 值加入多少文件 (*依國家/地區的點擊數*)?

- 過去 15 分鐘內，哪 15 個組織最常有文件加入 (*前 15 名組織*)?

- 過去 15 分鐘內，多少不同組織有文件加入 (*唯一計數組織*)?

- 過去 15 分鐘加入多少個文件 (*點擊總數*)?

- 過去 15 分鐘內，多少不同 *SourceIp* 值有文件加入 (*唯一 IP 計數*)?


索引的定義和查詢的詳細資料概述於[附錄](#appendix-the-query-and-aggregation-performance-test)。

設計這些測試是為了了解下列變數的效果：

- **磁碟類型**。測試是在使用標準儲存體 (HDD) 的 D4 VM 組成的 6 個節點叢集上執行，並在使用進階儲存體 (SSD) 的 DS4 VM 組成的 6 節點叢集上重複測試。

- **機器大小 - 向上擴充**。測試是在由 DS3 VM 組成的 6 節點叢集 (*小型*叢集) 上執行，並分別在 DS4 VM 的叢集 (*中型*叢集) 和 DS14 機器的叢集 (*大型*叢集) 上重複測試。下表摘要說明每個 VM SKU 的主要特性：

 叢集 | VM SKU | 核心數目 | 資料磁碟數量 | RAM (GB) |
---------|---------------|-----------------|----------------------|----------|
 小型 | 標準 DS3 | 4 | 8 | 14 |
 中型 | 標準 DS4 | 8 | 16 | 28 |
 大型 | 標準 DS14 | 16 | 32 | 112 |

- **叢集大小 - 相應放大**。測試是在包含 1、3 和 6 個節點的 DS14 VM 叢集上執行。

- **索引複本數目**。測試是使用設定 1 和 2 個複本的索引執行。

- **文件值**。一開始是以 doc\_values 設為 true (預設值) 的索引執行測試。再將 doc\_values 設為 false，重複執行選取的測試。

- **快取**。測試進行時在索引上已啟用分區要求快取。

- **分區數目**。測試使用不同分區數來確認查詢在包含較少、較大分區的索引執行較有效率，還是包含較多、較小分區的索引。


## 效能結果 – 磁碟類型

磁碟效能的評估是藉由在 D4 VM (使用 HDD) 組成的 6 節點叢集上以及在 DS4 VM (使用 SSD) 組成的 6 節點叢集上執行*擷取和查詢*測試。兩個叢集的 Elasticsearch 組態相同。資料已分散到每個節點的 16 個磁碟上，且每個節點有 14 GB RAM 配置給執行 Elasticsearch 的 JVM；剩餘的記憶體 (也就是 14 GB) 是保留供作業系統使用。每項測試執行 24 小時。選擇這個時間長度，是為了讓資料量增加的效果更加明顯，以及讓系統穩定。下表摘要說明結果，並且反白顯示組成測試之各種作業的回應時間。

 叢集 | 作業/查詢 | 平均回應時間 (毫秒) |
---------|----------------------------|----------------------------|
 D4 | 擷取 | 978 |
 | 依評等的計數 | 103 |
 | 不同時間的計數 | 134 |
 | 依國家/地區的點擊數 | 199 |
 | 前 15 名組織 | 137 |
 | 唯一計數組織 | 139 |
 | 唯一的 IP 計數 | 510 |
 | 點擊總數 | 89
 DS4 | 擷取 | 511 |
 | 依評等的計數 | 187 |
 | 不同時間的計數 | 411 |
 | 依國家/地區的點擊數 | 402 |
 | 前 15 名組織 | 307 |
 | 唯一計數組織 | 320 |
 | 唯一的 IP 計數 | 841 |
 | 點擊總數 | 236 |

乍看之下，DS4 叢集執行的查詢似乎比 D4 叢集少，且回應時間加倍 (或較差)。不過這只是表面的結果。下表顯示每個叢集執行的擷取作業數目 (請記住，每個作業會載入 1000 個文件)：

 叢集 | 擷取作業數 |
---------|-------------------------|
 D4 | 264769 |
 DS4 | 503157 |

在測試期間，DS4 叢集能夠載入的資料幾乎是 D4 叢集中的兩倍。因此，在分析每個作業的回應時間時，您還必須考慮每個查詢必須掃描多少文件的，以及傳回多少文件。這些數字不是一成不變的，因為索引中文件的量會持續成長。您不能只是將 503137 除以 264769 (每個叢集執行的擷取作業數目)，將結果乘以 D4 叢集執行每個查詢的平均回應時間，計算出比較資訊，因為這樣的計算結果忽略了擷取作業同時執行的 I/O 數量。相反地，您應該測量測試期間寫入磁碟以及從磁碟讀取的實體資料量。JMeter 測試計劃會為每個節點擷取這項資訊。結果摘要如下：

 叢集 | 每項作業平均寫入/讀取位元組 |
---------|----------------------------------------------|
 D4 | 13471557 |
 DS4 | 24643470 |

此資料顯示 DS4 叢集大約能夠承受D4 叢集的 1.8 倍 I/O 速率。基於這個結果，除了磁碟的本質外，其他所有資源都相同，則差異必定是來自使用 SSD 而非 HDD。

為了證明這個結論，下圖說明每個叢集在不同時間如何執行 I/O：

![](./media/guidance-elasticsearch/query-performance2.png)

<!-- -->

***圖 2.D4 和 DS4 叢集的磁碟活動***

D4 叢集的圖顯示重大變化，特別是在測試的前半段。這可能是因為系統採取節流來降低 I/O 速率。在測試的初始階段，查詢可以快速地執行，因為只有一些資料要分析。因此，D4 叢集中磁碟的運作可能很接近其 IOPS 能力，雖然每個 I/O 作業可能不會傳回許多資料。DS4 叢集能夠支援較高的 IOPS 速率，且不會發生相同程度的節流；I/O 速度比較規則。為了支援這個理論，下兩張圖顯示磁碟 I/O在不同時間如何封鎖 CPU (圖中的磁碟等候時間是 CPU 等待 I/O 所花費時間的比例)：

![](./media/guidance-elasticsearch/query-performance3.png)

***圖 3.D4 和 DS4 叢集的 CPU 磁碟 I/O 等候時間***

請務必了解，I/O 作業封鎖 CPU 有兩個主要原因：

- I/O 子系統無法從磁碟讀取資料或將資料寫入磁碟。

- 主機環境可能會節流 I/O 子系統。使用 HDD 實作的 Azure 磁碟最大輸送量為 500 IOPS，SSD 的最大輸送量則為 5000 IOPS。


在前半段測試期間，D4 叢集花在等候 I/O 的時間，與圖形顯示的 I/O 速率反向密切相關；低 I/O 的期間對應到 CPU 被封鎖的一段長時間；這表示系統正在節流 I/O。隨著更多的資料加入叢集，情況跟著改變，在後半段的測試中，尖峰 I/O 等候時間對應至尖峰 I/O 輸送量。此時，CPU 被封鎖，同時執行真正的 I/O。同樣地，DS4 叢集用在等候 I/O 所花費的時間較平均，且每個尖峰在 I/O 效能都有對等的尖峰 (而不是低谷)；這意味著幾乎沒有發生任何節流。

還有一個因素需要考慮。在測試期間，D4 叢集產生 10584 擷取錯誤和 21 查詢錯誤。在 DS4 叢集上的測試沒有產生任何錯誤。

## 效能結果 – 相應增加

相應增加測試的執行是對 DS3、DS4 和 DS14 VM 的 6 節點叢集執行測試。之所以選取這些 SKU，是因為 DS4 VM 提供的 CPU 核心和記憶體是 DS3 的兩倍，而 DS14 機器的 CPU 資源則再次翻倍，同時提供四倍的記憶體數量。下表比較每個 SKU 的重要層面：

 SKU | CPU 核心數 | 記憶體 (GB) | 最大磁碟 IOPS | 最大頻寬 (MB/s)|
------|-------------|-------------|---------------|--------------|
 DS3 | 4 | 14 | 12,800| 128 |
 DS4 | 8 | 28 | 25,600| 256 |
 DS14 | 16 | 112 | 50,000| 512 |

下表摘要說明在小型 (DS3)、中型 (DS4) 和大型 (DS14) 叢集上執行測試的結果。每個 VM 使用 SSD 來保存資料。每項測試執行 24 小時：

> **注意**：資料表會報告每一種查詢成功的要求數目 (不包含失敗)。針對每種類型的查詢嘗試的要求數目在測試回合期間大致上相同。這是因為 JMeter 測試計劃會單次執行每個查詢 (依評等的計數、不同時間的計數、依國家/地區的點擊數、前 15 名組織、唯一計數組織、唯一的 IP 計數，和點擊總數)，全都在單一單位中，稱為*測試交易* (此交易獨立於擷取作業的工作，擷取作業由個別的執行緒所執行)。測試計劃的每次反覆會執行單一測試交易。因此，已完成的測試交易數目是每個交易中最慢查詢的回應時間量值。

| 叢集 | 作業/查詢 | 要求數目 | 平均回應時間 (毫秒) |
|--------------|----------------------------|--------------------|----------------------------|
| 小型 (DS3) | 擷取 | 207284 | 3328 |
| | 依評等的計數 | 18444 | 268 |
| | 不同時間的計數 | 18444 | 340 |
| | 依國家/地區的點擊數 | 18445 | 404 |
| | 前 15 名組織 | 18439 | 323 |
| | 唯一計數組織 | 18437 | 338 |
| | 唯一的 IP 計數 | 18442 | 468 |
| | 點擊總數 | 18428 | 294   
|||||
| 中型 (DS4) | 擷取 | 503157 | 511 |
| | 依評等的計數 | 6958 | 187 |
| | 不同時間的計數 | 6958 | 411 |
| | 依國家/地區的點擊數 | 6958 | 402 |
| | 前 15 名組織 | 6958 | 307 |
| | 唯一計數組織 | 6956 | 320 |
| | 唯一的 IP 計數 | 6955 | 841 |
| | 點擊總數 | 6958 | 236 |
|||||
| 大型 (DS14) | 擷取 | 502714 | 511 |
| | 依評等的計數 | 7041 | 201 |
| | 不同時間的計數 | 7040 | 298 |
| | 依國家/地區的點擊數 | 7039 | 363 |
| | 前 15 名組織 | 7038 | 244 |
| | 唯一計數組織 | 7037 | 283 |
| | 唯一的 IP 計數 | 7037 | 681 |
| | 點擊總數 | 7038 | 200 |

這些數字顯示，對於這項測試，DS4 和 DS14 叢集的效能相當類似。DS3 叢集的查詢作業回應時間也似乎比起一開始時正面，且執行的查詢作業數目遠超過 DS4 和 DS14 叢集的值。不過，您也應該特別注意擷取速率和後續搜尋的文件數。DS3 叢集擷取受到的限制高出許多，在測試結束時，資料庫只會包含其他兩個叢集各自讀入的大約 40% 文件。這可能是由於 DS3 VM 和 DS4 或 DS14 VM 相比，可用的處理資源、網路和磁碟頻寬。假設 DS4 VM 的資源為 DS3 VM 的兩倍，DS14 的資源為 DS4 VM 的兩倍 (記憶體為四倍)，仍然有一個問題：為何 DS4 和 DS14 叢集之間的擷取速率差異大幅小於 DS3 和 DS4 叢集之間的擷取速率？ 這可能是因為 Azure VM 的網路用量與頻寬限制。下圖顯示所有三個叢集的這項資料：

![](./media/guidance-elasticsearch/query-performance4.png)

***圖 4.執行*擷取和查詢*測試之 DS3、DS4 和 DS14 叢集的網路使用率***

<!-- -->

Azure VM 的可用網路頻寬限制不會發佈且可能會不同，但 DS4 和 DS14 測試中，網路活動似乎在平均大約 2.75GBps 達到穩定的事實，暗示已經達到這類限制，而且已經成為限制輸送量的主要因素。在 DS3 叢集的案例中，網路活動大幅降低，因此較低的效能更可能是因為其他資源的可用性條件約束。

若要隔離擷取作業的效果，並說明查詢效能如何隨著節點相應增加而變化，使用了相同的節點執行一組僅查詢的測試。下表摘要說明每一個叢集上所得到的結果：

> [AZURE.NOTE] 您不應該拿*僅查詢*測試中的查詢效能和所執行的要求數，與*擷取和查詢*測試執行的來比較。這是因為查詢已經過修改，相關的相關文件量不同。

| 叢集 | 作業/查詢 | 要求數目 | 平均回應時間 (毫秒) |
|--------------|----------------------------|--------------------|----------------------------|
| 小型 (DS3) | 依評等的計數 | 464 | 11758 |
| | 不同時間的計數 | 464 | 14699 |
| | 依國家/地區的點擊數 | 463 | 14075 |
| | 前 15 名組織 | 464 | 11856 |
| | 唯一計數組織 | 462 | 12314 |
| | 唯一的 IP 計數 | 461 | 19898 |
| | 點擊總數 | 462 | 8882  
|||||
| 中型 (DS4) | 依評等的計數 | 1045 | 4489 |
| | 不同時間的計數 | 1045 | 7292 |
| | 依國家/地區的點擊數 | 1053 | 7564 |
| | 前 15 名組織 | 1055 | 5066 |
| | 唯一計數組織 | 1051 | 5231 |
| | 唯一的 IP 計數 | 1051 | 9228 |
| | 點擊總數 | 1051 | 2180 |
|||||
| 大型 (DS14) | 依評等的計數 | 1842 | 1927 |
| | 不同時間的計數 | 1839 | 4483 |
| | 依國家/地區的點擊數 | 1838 | 4761 |
| | 前 15 名組織 | 1842 | 2117 |
| | 唯一計數組織 | 1837 | 2393 |
| | 唯一的 IP 計數 | 1837 | 7159 |
| | 點擊總數 | 1837 | 642 |

這次，不同叢集之間的平均回應時間趨勢更清楚。網路使用率完全低於稍早 DS4 和 DS14 叢集所需的 2.75GBps (這可能會在擷取和查詢測試中使網路達到飽和)，和 DS3 叢集的 1.5GBps。事實上，在所有情況下比較接近 200MBps，如下列圖表所示：

![](./media/guidance-elasticsearch/query-performance5.png)

***圖 5.執行*僅查詢*測試之 DS3、DS4 和 DS14 叢集的網路使用率***

DS3 和 DS4 叢集中的限制因素現在似乎是 CPU 使用率，這在許多時間都是接近 100%。在 DS14 叢集中，CPU 使用量平均剛好超過 80%。這仍然很高，但顯然會突顯擁有更多可用 CPU 核心的優點。下圖說明 DS3、DS4 和 DS14 叢集的 CPU 使用量模式。

![](./media/guidance-elasticsearch/query-performance6.png)

***圖 6.執行*僅查詢*測試之 DS3 和 DS14 叢集的網路使用率***

## 效能結果 – 相應放大

為了說明系統的節點數如何相應放大，使用了包含 1、3 和 6 個節點的 DS14 叢集來執行測試。這次，只執行了*僅查詢*測試，且使用 1 億份文件並執行 90 分鐘：

> [AZURE.NOTE] 如需相應放大對資料擷取作業行為之影響的詳細資訊，請參閱文件[有利用 Azure 上的 Elasticsearch 最佳化資料擷取效能](https://github.com/mspnp/azure-guidance/blob/master/Elasticsearch-Data-Ingestion-Performance.md)。

| 叢集 | 作業/查詢 | 要求數目 | 平均回應時間 (毫秒) |
|---------|----------------------------|--------------------|----------------------------|
| 1 個節點 | 依評等的計數 | 288 | 6216 |
| | 不同時間的計數 | 288 | 28933 |
| | 依國家/地區的點擊數 | 288 | 29455 |
| | 前 15 名組織 | 288 | 9058 |
| | 唯一計數組織 | 287 | 19916 |
| | 唯一的 IP 計數 | 284 | 54203 |
| | 點擊總數 | 287 | 3333 |
|||||
| 3 個節點 | 依評等的計數 | 1194 | 3427 |
| | 不同時間的計數 | 1194 | 5381 |
| | 依國家/地區的點擊數 | 1191 | 6840 |
| | 前 15 名組織 | 1196 | 3819 |
| | 唯一計數組織 | 1190 | 2938 |
| | 唯一的 IP 計數 | 1189 | 12516 |
| | 點擊總數 | 1191 | 1272 |
|||||
| 6 個節點 | 依評等的計數 | 1842 | 1927 |
| | 不同時間的計數 | 1839 | 4483 |
| | 依國家/地區的點擊數 | 1838 | 4761 |
| | 前 15 名組織 | 1842 | 2117 |
| | 唯一計數組織 | 1837 | 2393 |
| | 唯一的 IP 計數 | 1837 | 7159 |
| | 點擊總數 | 1837 | 642 |

節點數目對叢集的查詢效能造成顯著的差異，不過是非線性的方式；3 個節點的叢集比起單一節點的叢集，會完成大約 4 倍的查詢，而 6 個節點的叢集則處理最多 6 倍。為了協助說明這個非線性，下圖顯示三個叢集對 CPU 的使用情形：

![](./media/guidance-elasticsearch/query-performance7.png)

***圖 7.執行*僅查詢*測試之 1、3 和 6 節點叢集的網路使用率***

單一節點和 3 個節點的叢集是 CPU-bound，而在 6 個節點叢集中，雖然 CPU 使用率很高，仍有備用的處理容量。在此情況下，其他的因素很可能會限制輸送量。這可以藉由使用 9 和 12 個節點進行測試來確認，這樣的測試可能會顯示進一步的備用處理容量。

上表中的資料也會示範查詢的平均回應時間變化情形。這是針對特定查詢類型測試系統調整方式時，最有用的資訊項目；在跨越多個節點，有些搜尋明顯比其他搜尋更有效率許多。這可能是因為節點數目和叢集中的文件數目的比率增加；每個叢集包含 1 億份文件。在執行時牽涉到資料彙總的搜尋時，Elasticsearch 會處理並緩衝在每個節點的記憶體彙總程序一部分中擷取到的資料。如果有更多個節點，則在每個節點上要擷取、緩衝和處理的資料就更少。

## 效能結果 - 複本數目

*擷取和查詢*測試針對具有單一複本的索引執行。使用設定兩個複本的索引在 6 個節點的 DS4 和 DS14 叢集上重複執行了測試。所有測試皆執行 24 小時。下表顯示一個與兩個複本的比較結果：

| 叢集 | 作業/查詢 | 平均回應時間 (毫秒) - 1 個複本 | 平均回應時間 (毫秒) - 2 個複本 | 回應時間的差異 % |
|---------|----------------------------|----------------------------------------|-----------------------------------------|-------------------------------|
| DS4 | 擷取 | 511 | 655 | +28% |
| | 依評等的計數 | 187 | 168 | -10% |
| | 不同時間的計數 | 411 | 309 | -25% |
| | 依國家/地區的點擊數 | 402 | 562 | +40% |
| | 前 15 名組織 | 307 | 366 | +19% |
| | 唯一計數組織 | 320 | 378 | +18% |
| | 唯一的 IP 計數 | 841 | 987 | +17% |
| | 點擊總數 | 236 | 236 | +0% |
||||||
| DS14 | 擷取 | 511 | 618 | +21% |
| | 依評等的計數 | 201 | 275 | +37% |
| | 不同時間的計數 | 298 | 466 | +56% |
| | 依國家/地區的點擊數 | 363 | 529 | +46% |
| | 前 15 名組織 | 244 | 407 | +67% |
| | 唯一計數組織 | 283 | 403 | +42% |
| | 唯一的 IP 計數 | 681 | 823 | +21% |
| | 點擊總數 | 200 | 221 | +11% |

擷取速率隨著複本數目增加而降低。這是預期現象，因為 Elasticsearch 針對每份文件撰寫更多份，產生額外的磁碟 I/O。這反映在索引具有 1 個和 2 個複本的 DS14 叢集圖形，如下圖所示。在具有 1 個複本的索引案例，平均 I/O 速率是 16896573 位元組/秒。對於具有 2 個複本的索引，平均 I/O 速率是 33986843 位元組/秒；剛好差不多兩倍。

![](./media/guidance-elasticsearch/query-performance8.png)

***圖 8.具有 1 個和 2 個複本執行*擷取和查詢*測試的節點磁碟 I/O 速率***

| 叢集 | 查詢 | 平均回應時間 (毫秒) - 1 個複本 | 平均回應時間 (毫秒) - 2 個複本 |
|---------|----------------------------|----------------------------------------|-----------------------------------------|
| DS4 | 依評等的計數 | 4489 | 4079 |
| | 不同時間的計數 | 7292 | 6697 |
| | 依國家/地區的點擊數 | 7564 | 7173 |
| | 前 15 名組織 | 5066 | 4650 |
| | 唯一計數組織 | 5231 | 4691 |
| | 唯一的 IP 計數 | 9228 | 8752 |
| | 點擊總數 | 2180 | 1909 |
|||||
| DS14 | 依評等的計數 | 1927 | 2330 |
| | 不同時間的計數 | 4483 | 4381 |
| | 依國家/地區的點擊數 | 4761 | 5341 |
| | 前 15 名組織 | 2117 | 2560 |
| | 唯一計數組織 | 2393 | 2546 |
| | 唯一的 IP 計數 | 7159 | 7048 |
| | 點擊總數 | 642 | 708 |

這些結果顯示 DS4 叢集的平均回應時間有改進，但 DS14 叢集的則增加。為了協助解譯這些結果，您也應該考慮每項測試所執行的查詢數目：

| 叢集 | 查詢 | 執行數 - 1 個複本 | 執行數 - 2 個複本 |
|---------|----------------------------|------------------------------|-------------------------------|
| DS4 | 依評等的計數 | 1054 | 1141 |
| | 不同時間的計數 | 1054 | 1139 |
| | 依國家/地區的點擊數 | 1053 | 1138 |
| | 前 15 名組織 | 1055 | 1141 |
| | 唯一計數組織 | 1051 | 1136 |
| | 唯一的 IP 計數 | 1051 | 1135 |
| | 點擊總數 | 1051 | 1136 |
|||||
| DS14 | 依評等的計數 | 1842 | 1718 |
| | 不同時間的計數 | 1839 | 1716 |
| | 依國家/地區的點擊數 | 1838 | 1714 |
| | 前 15 名組織 | 1842 | 1718 |
| | 唯一計數組織 | 1837 | 1712 |
| | 唯一的 IP 計數 | 1837 | 1712 |
| | 點擊總數 | 1837 | 1712 |

這項資料顯示 DS4 叢集所執行的查詢數目增加，會與平均回應時間的減少一致，但同樣地，DS14 叢集則是相反的情況。其中一個重要因素是在 1 個複本和 2 個複本的測試中，DS4 叢集的 CPU 使用率分佈並不平均；某些節點顯示接近 100% 的使用率，其他的則有備用的處理容量。效能提升很可能是因為在叢集的節點之間分散處理的能力增加。下圖顯示使用量最少與最多的 VM (節點 4 和 3) 之間，CPU 處理的變化：

![](./media/guidance-elasticsearch/query-performance9.png)

***圖 9.執行*僅查詢*測試的 DS4 叢集中，最少使用與最常使用的節點的 CPU 使用率***

DS14 叢集的情形不是如此。兩項測試的 CPU 使用率在所有節點間都較低，且第二個複本的可用性變成更不像是一項優點，而更像額外負荷：

![](./media/guidance-elasticsearch/query-performance10.png)

***圖 10.執行*僅查詢*測試的 DS14 叢集中，最少使用與最常使用的節點的 CPU 使用率***

這些結果顯示，當您決定是否要使用多個複本時，需要仔細基準測試您的系統。除非您願意承擔節點失敗時遺失資料的風險，否則應該永遠有每個索引的至少一個複本，但額外的複本可能造成系統負擔而沒有多少好處，視您的工作負載和可供叢集使用的硬體資源而定。

## 效能結果 – 文件值

在啟用文件值的情況下執行*擷取和查詢*測試，導致 Elasticsearch 儲存用來排序磁碟上欄位的資料。接著，再停用文件值重複測試，因此 Elasticsearch 會動態建構 fielddata 並在記憶體中快取它。所有測試皆執行 24 小時。下表比較針對使用 D4、DS4 和 DS14 VM 建置之 6 個節點的叢集執行的測試的回應時間 (D4 叢集使用一般硬碟，而 DS4 和 DS14 叢集使用 SSD)。

| 叢集 | 作業/查詢 | 平均回應時間 (毫秒) - 已啟用文件值 | 平均回應時間 (毫秒) - 已停用文件值 | 回應時間的差異 % |
|---------|----------------------------|-------------------------------------------------|--------------------------------------------------|-------------------------------|
| D4 | 擷取 | 978 | 835 | -15% |
| | 依評等的計數 | 103 | 132 | +28% |
| | 不同時間的計數 | 134 | 189 | +41% |
| | 依國家/地區的點擊數 | 199 | 259 | +30% |
| | 前 15 名組織 | 137 | 184 | +34% |
| | 唯一計數組織 | 139 | 197 | +42% |
| | 唯一的 IP 計數 | 510 | 604 | +18% |
| | 點擊總數 | 89 | 134 | +51% |
||||||
| DS4 | 擷取 | 511 | 581 | +14% |
| | 依評等的計數 | 187 | 190 | +2% |
| | 不同時間的計數 | 411 | 409 | -0.5% |
| | 依國家/地區的點擊數 | 402 | 414 | +3% |
| | 前 15 名組織 | 307 | 284 | -7% |
| | 唯一計數組織 | 320 | 313 | -2% |
| | 唯一的 IP 計數 | 841 | 955 | +14% |
| | 點擊總數 | 236 | 281 | +19% |
||||||
| DS14 | 擷取 | 511 | 571 | +12% |
| | 依評等的計數 | 201 | 232 | +15% |
| | 不同時間的計數 | 298 | 341 | +14% |
| | 依國家/地區的點擊數 | 363 | 457 | +26% |
| | 前 15 名組織 | 244 | 338 | +39% |
| | 唯一計數組織 | 283 | 350 | +24% |
| | 唯一的 IP 計數 | 681 | 909 | +33% |
| | 點擊總數 | 200 | 245 | +23% |

下表比較測試所執行的擷取作業數目：

| 叢集 | 擷取作業數 - 已啟用文件值 | 擷取作業數 - 已停用文件值 | 擷取作業數的差異 % |
|---------|----------------------------------------------|-----------------------------------------------|-----------------------------------------|
| D4 | 264769 | 408690 | +54% |
| DS4 | 503137 | 578237 | +15% |
| DS14 | 502714 | 586472 | +17% |

已停用文件值時會發生擷取速率改善的情況，因為在插入文件時會有較少資料寫入到磁碟。效能改善在 D4 VM 用來 HDD 來儲存資料時特別明顯。在此情況下，擷取作業的回應時間也減少 15% (請參閱本節中的第一個資料表)。這可能是因為 HDD 上的壓力減輕，HDD 可能在啟用文件值的測試中已接近其 IOPS 限制；如需詳細資訊，請參閱磁碟類型測試。下圖比較 D4 VM 在啟用文件值時 (值保留在磁碟上) 和停用文件值時 (值保留在記憶體中) 的 I/O 效能：

![](./media/guidance-elasticsearch/query-performance11.png)

***圖 11.啟用和停用文件值時的 D4 叢集磁碟活動***

相反地，使用 SSD 之 VM 的擷取值顯示文件數少量增加，但擷取作業的回應時間也同時增加。有一或兩個小例外，查詢回應時間也會較差。啟用文件值時，SSD 比較不會執行接近其 IOPS 限制，因此效能變更更有可能是因為處理活動增加，以及管理 JVM 堆積的額外負荷。藉由比較啟用和停用文件值時的 CPU 使用率即可證明此點。下個圖形突顯 DS4 叢集的這項資料，其中大部分的 CPU 使用率從啟用文件值的 30%-40% 級區，移到停用文件值的 40%-50% 級區 (DS14 叢集顯示類似的趨勢)：

![](./media/guidance-elasticsearch/query-performance12.png)

***圖 12.啟用和停用文件值時的 DS4 叢集 CPU 使用率***

為了區分文件值對資料擷取查詢效能的效果，因此對 DS4 和 DS14 叢集且啟用和停用文件值的情況執行了成對的僅查詢測試。下表摘要說明這些測試的結果：

| 叢集 | 作業/查詢 | 平均回應時間 (毫秒) - 已啟用文件值 | 平均回應時間 (毫秒) - 已停用文件值 | 回應時間的差異 % |
|---------|----------------------------|-------------------------------------------------|--------------------------------------------------|-------------------------------|
| DS4 | 依評等的計數 | 4489 | 3736 | -16% |
| | 不同時間的計數 | 7293 | 5459 | -25% |
| | 依國家/地區的點擊數 | 7564 | 5930 | -22% |
| | 前 15 名組織 | 5066 | 3874 | -14% |
| | 唯一計數組織 | 5231 | 4483 | -2% |
| | 唯一的 IP 計數 | 9228 | 9474 | +3% |
| | 點擊總數 | 2180 | 1218 | -44% |
||||||
| DS14 | 依評等的計數 | 1927 | 2144 | +11% |
| | 不同時間的計數 | 4483 | 4337 | -3% |
| | 依國家/地區的點擊數 | 4761 | 4840 | +2% |
| | 前 15 名組織 | 2117 | 2302 | +9% |
| | 唯一計數組織 | 2393 | 2497 | +4% |
| | 唯一的 IP 計數 | 7159 | 7639 | +7% |
| | 點擊總數 | 642 | 633 | -1% |

請記住，Elasticsearch 2.0 及之後的版本預設會啟用文件值。在涵蓋 DS4 叢集的測試中，停用文件值似乎有正面的整體影響，對於 DS14 叢集則是一般皆為相反情形 (兩種情況下，停用文件值的效能較好，但程度很低)。

對於 DS4 叢集，兩種情況的 CPU 使用率在兩個測試的持續時間都接近 100%，表示叢集 CPU-bound。不過，處理查詢數從 7369 減少為 5894 (20%)？請參閱下表。請記住，如果停用文件值，Elasticsearch 會在記憶體中以動態方式產生 fielddata，這會消耗 CPU 能力。這項組態可降低磁碟 I/O 率，但對於已接近其最大容量的 CPU 會造成更大的壓力，因此在此情況下，停用文件值時查詢速度較快，但查詢數較少。

在不論是否有文件值的 DS14 測試中，CPU 活動都很高，但不是 100%。在啟用文件值的測試中，執行的查詢數目稍微較高 (大約 4%)：

| 叢集 | 查詢 | 執行數 - 已啟用文件值 | 執行數 - 已停用文件值 |
|---------|----------------------------|---------------------------------------|----------------------------------------|
| DS4 | 依評等的計數 | 1054 | 845 |
| | 不同時間的計數 | 1054 | 844 |
| | 依國家/地區的點擊數 | 1053 | 842 |
| | 前 15 名組織 | 1055 | 846 |
| | 唯一計數組織 | 1051 | 839 |
| | 唯一的 IP 計數 | 1051 | 839 |
| | 點擊總數 | 1051 | 839  
||||| |
| DS14 | 依評等的計數 | 1772 | 1842 |
| | 不同時間的計數 | 1772 | 1839 |
| | 依國家/地區的點擊數 | 1770 | 1838 |
| | 前 15 名組織 | 1773 | 1842 |
| | 唯一計數組織 | 1769 | 1837 |
| | 唯一的 IP 計數 | 1768 | 1837 |
| | 點擊總數 | 1769 | 1837 |

## 效能結果 - 分區要求快取

為了示範每個節點的記憶體中，快取索引資料可能對效能有什麼影響，已在 DS4 和 DS14 6 個節點的叢集上進行*擷取和查詢*測試，並啟用索引快取 - 如需詳細資訊，請參閱[使用分區要求快取](#using-the-shard-request-cache)一節。結果和使用相同索引但停用索引快取的稍早測試所產生的結果相比較。下表摘要說明結果。請注意，資料已遭到削減，只涵蓋測試的前 90 分鐘。此時比較趨勢已經明顯，繼續進行測試可能不會產生任何額外的見解：

| 叢集 | 作業/查詢 | 平均回應時間 (毫秒) - 已停用索引快取 | 平均回應時間 (毫秒) - 已啟用索引快取 | 回應時間的差異 % |
|---------|----------------------------|---------------------------------------------------|--------------------------------------------------|-------------------------------|
| DS4 | 擷取 | 504 | 3260 | +547% |
| | 依評等的計數 | 218 | 273 | +25% |
| | 不同時間的計數 | 450 | 314 | -30% |
| | 依國家/地區的點擊數 | 447 | 397 | -11% |
| | 前 15 名組織 | 342 | 317 | -7% |
| | 唯一計數組織 | 370 | 324 | -12%% |
| | 唯一的 IP 計數 | 760 | 355 | -53% |
| | 點擊總數 | 258 | 291 | +12% |
||||||
| DS14 | 擷取 | 503 | 3365 | +569% |
| | 依評等的計數 | 234 | 262 | +12% |
| | 不同時間的計數 | 357 | 298 | -17% |
| | 依國家/地區的點擊數 | 416 | 383 | -8% |
| | 前 15 名組織 | 272 | 324 | -7% |
| | 唯一計數組織 | 330 | 321 | -3% |
| | 唯一的 IP 計數 | 674 | 352 | -48% |
| | 點擊總數 | 227 | 292 | +29% |

這項資料顯示兩個重點：

-  資料擷取速率似乎會因為啟用索引快取而大幅降低，以及

-  索引快取不一定改善所有類型之查詢的回應時間，而且可能對特定彙總作業有不良的影響，例如依評等的計數和點擊總數所執行的作業。
 

若要了解系統為何展現此行為，您應該考慮測試執行期間，每個案例中成功執行的查詢數目。下表摘要說明這項資料：

| 叢集 | 作業/查詢 | 作業/查詢數 - 已停用索引快取 | 作業/查詢數 - 已啟用索引快取 |
|---------|----------------------------|-------------------------------------------------|------------------------------------------------|
| DS4 | 擷取 | 38611 | 13232 |
| | 依評等的計數 | 524 | 18704 |
| | 不同時間的計數 | 523 | 18703 |
| | 依國家/地區的點擊數 | 522 | 18702 |
| | 前 15 名組織 | 521 | 18706 |
| | 唯一計數組織 | 521 | 18700 |
| | 唯一的 IP 計數 | 521 | 18699 |
| | 點擊總數 | 521 | 18701  
|||| |
| DS14 | 擷取 | 38769 | 12835 |
| | 依評等的計數 | 528 | 19239 |
| | 不同時間的計數 | 528 | 19239 |
| | 依國家/地區的點擊數 | 528 | 19238 |
| | 前 15 名組織 | 527 | 19240 |
| | 唯一計數組織 | 524 | 19234 |
| | 唯一的 IP 計數 | 524 | 19234 |
| | 點擊總數 | 527 | 19236 |

您可以看到雖然啟用快取時擷取速率大約是停用快取時的 1/3，執行的查詢數目會增加 34 倍。查詢不會再產生一樣多的磁碟 I/O，且不必競爭磁碟資源。這反映在底下的圖 13 中圖形，圖形會比較所有四個案例的 I/O 活動：

![](./media/guidance-elasticsearch/query-performance13.png)

***圖 13.*擷取和查詢*測試已停用和啟用索引快取時的磁碟 I/O 活動***

磁碟 I/O 的減少也表示 CPU 花費更少的時間等候 I/O 完成。這由圖 14 所突顯：

![](./media/guidance-elasticsearch/query-performance14.png)

***圖 14. *擷取和查詢*測試已停用和啟用索引快取時，花費在等待磁碟 I/O 完成的 CPU 時間***

磁碟 I/O 的減少表示 Elasticsearch 可以花費更大比例的時間在處理保留在記憶體中之資料的查詢。這會增加 CPU 使用率，如果您查看所有的四種情況下的 CPU 使用率便會變得顯而易見。底下的圖形顯示啟用快取時，CPU 使用如何更能維持：

![](./media/guidance-elasticsearch/query-performance15.png)

***圖 15.*擷取和查詢*測試已停用和啟用索引快取時的 CPU 使用率***

在兩個案例中，測試期間的網路 I/O 量廣泛地類似。沒有快取的測試在測試期間會顯示逐漸降低，但這些測試較長的 24 小時執行則顯示這項統計資料在大約 2.75GBps 處達到穩定。下圖顯示 DS4 叢集的這項資料 (DS14 叢集的資料非常類似)：

![](./media/guidance-elasticsearch/query-performance16.png)

***圖 16.*擷取和查詢*測試已停用和啟用索引快取時的網路流量***

如[相應增加](#performance-results-scaling-up)測試中所述，Azure VM 的網路頻寬限制不會發佈且可能會不同，但一般層級的 CPU 和磁碟活動暗示網路使用率可能是此案例中的限制因素。

快取較自然地適合資料不常變更的案例。為了突顯此案例中快取的影響，已啟用快取，執行*僅查詢*測試。結果如下所示 (這些測試執行 90 分鐘，且受測索引包含 1 億份文件)：

| 叢集 | 查詢 | 平均回應時間 (毫秒) | 執行的查詢數目 |
|---------|----------------------------|----------------------------|-------------------------|
| | | **已停用快取** | **已啟用快取** |
| DS4 | 依評等的計數 | 4489 | 210 |
| | 不同時間的計數 | 7292 | 211 |
| | 依國家/地區的點擊數 | 7564 | 231 |
| | 前 15 名組織 | 5066 | 211 |
| | 唯一計數組織 | 5231 | 211 |
| | 唯一的 IP 計數 | 9228 | 218 |
| | 點擊總數 | 2180 | 210  
|||| |
| DS14 | 依評等的計數 | 1927 | 211 |
| | 不同時間的計數 | 4483 | 219 |
| | 依國家/地區的點擊數 | 4761 | 236 |
| | 前 15 名組織 | 2117 | 212 |
| | 唯一計數組織 | 2393 | 212 |
| | 唯一的 IP 計數 | 7159 | 220 |
| | 點擊總數 | 642 | 211 |

非快取測試的效能差異，是因為 DS4 和 DS14 VM 之間可用的資源差異。在快取測試的兩種情況下，平均回應時間會大幅滑落，因為資料是直接從記憶體擷取。另外值得注意的是，快取 DS4 和 DS14 叢集測試的回應時間非常類似，儘管與非快取的結果不同。每個測試內的每個查詢回應時間差異也很小；它們全都需要大約 220ms。兩個叢集的磁碟 I/O 速率和 CPU 使用率都很低，因為一旦所有資料在記憶體中，就只需要很少的 I/O 或處理。網路 I/O 速率與未快取測試類似，確認網路頻寬可能是這項測試的限制因素。下圖顯示 DS4 叢集的這項資訊。DS14 叢集的情況非常類似：

![](./media/guidance-elasticsearch/query-performance17.png)

***圖 17.*僅查詢*測試已啟用索引快取時的磁碟 I/O、CPU 使用率和網路使用率***

上表中的數字暗示，使用 DS14 架構比起使用 DS4 並沒有什麼優勢。事實上，DS14 叢集所產生的樣本數大約低於 DS4 叢集 5%，但這也可能是因為在一段時間內非常輕微變化的網路限制。

## 效能結果 - 分區數目

這項測試的目的是要判斷針對索引建立的分區數目，對於該索引的查詢效能是否有任何關係。

先前進行的個別測試顯示索引的分區組態可能會影響資料擷取的速率。這些測試說明於文件[在 Azure 上使用 Elasticsearch 最大化資料擷取效能](https://github.com/mspnp/azure-guidance/blob/master/Elasticsearch-Data-Ingestion-Performance.md)。進行了測試以確定查詢效能遵循類似的方法，但限制在 DS14 硬體上執行的 6 個節點叢集。此方法有助於減少變數，因此效能的任何差異都應該是因為分區的量。

已在設定 7、 13、 23、 37 及 61 個主要分區的相同索引複本上進行*僅查詢*測試。索引包含 1 億份文件，而且具有單一本，叢集中的分區數目則加倍。每項測試執行 90 分鐘。下表摘要說明結果：顯示的平均回應時間，是 JMeter 測試異動的回應時間，此異動包含測試之每次反覆所執行的完整的查詢集。如需詳細資訊，請參閱[效能結果 – 相應增加](#performance-results-scaling-up)一節中的附註：

| 分區數目 | 分區版面配置 (每個節點的分區，包括複本) | 執行的查詢數目 | 平均回應時間 (毫秒) |
|---------------------------|----------------------------------------------------|-----------------------------|------------------------|
| 7 (包括複本為 14) | 3-2-2-2-2-3 | 7461 | 40524 |
| 13 (26) | 5-4-5-4-4-4 | 7369 | 41055 |
| 23 (46) | 7-8-8-7-8-8 | 14193 | 21283 |
| 37 (74) | 13-12-12-13-12-12 | 13399 | 22506 |
| 61 (122) | 20-21-20-20-21-20 | 14743 | 20445 |

這些結果表示 13(26) 分區叢集和 23(46) 分區叢集之間的效能有顯著的差異；輸送量幾乎是兩倍，而回應時間則為一半。這最可能是因為 Elasticsearch 用來處理搜尋要求的 VM 組態和結構。搜尋要求會排入佇列，而每個搜尋要求由單一搜尋執行緒處理。由 Elasticsearch 節點所建立的搜尋執行緒數目，是裝載節點的電腦上可用的處理器數目函式。結果暗示，節點上只有 4 或 5 個分區時，處理資源未被充分利用。執行此測試時查看 CPU 使用率即可支援此功能。下圖是執行 13(26) 分區測試時取自 Marvel 的快照集：

![](./media/guidance-elasticsearch/query-performance18.png)

***圖 18.7(14) 分區叢集上的*僅查詢*測試 CPU 使用率***

比較這些數字與 23(46) 分區測試的數字：

![](./media/guidance-elasticsearch/query-performance19.png)

***圖 19.23(46) 分區叢集上的*僅查詢*測試 CPU 使用率***

在 23(46) 分區測試中，CPU 使用率高出許多。每個節點包含 7 或 8 個分區。DS14 架構提供 16 個處理器，而且 Elasticsearch 更能夠利用這個數量的核心和其他分區。上表中的數字暗示，將分區數目增加超過此點可能會稍微改善效能，但您應該將這些數字與維護大量分區的額外負荷相抵消。這些測試的甜蜜點暗示每個節點的最佳分區數目是每個節點上可用的處理器核心數目的一半。不過，請記住，在只執行查詢時，已達到這些結果。如果您的系統會匯入資料，您也應該考慮分區化可能會對資料擷取作業的效能有何影響。如需此方面的進一步資訊，請參閱文件[在 Azure 上使用 Elasticsearch 最大化資料擷取效能](https://github.com/mspnp/azure-guidance/blob/master/Elasticsearch-Data-Ingestion-Performance.md)。

## 摘要

Elasticsearch 提供許多選項，您可以用來將索引結構化，並加以調整，以支援大規模的查詢作業。本文件摘要說明了一些常見的組態和技術，您可以用來針對查詢調整資料庫。不過，您應該在了解最佳化資料庫以支援快速擷取，而不是支援大量資料擷取之間有所取捨。有時適合查詢的可能會對插入作業有不利的影響，反之亦然。在處理混合工作負載的系統中，您需要評估平衡點在何處，並據以調整系統參數。

此外，不同的組態和技術的適用性，也可能視資料結構和建構系統時的硬體限制 (或其他) 有所不同。本文件中顯示的許多測試都說明硬體平台的選擇可能對輸送量有什麼影響，以及一些策略如何可能會有助於某些情況，但在其他情況中不利。重要的一點是了解可用的選項，然後再使用您自己的資料執行嚴格基準測試，判斷最佳的組合。

最後，請記住，Elasticsearch 資料庫不一定是靜態項目。它可能會隨著時間成長，用來建構資料的策略可能需要定期修改。例如，可能需要相應增加、相應放大或用其他分區重新建立資料的索引。隨著系統的大小和複雜度增加，請準備好持續測試效能，以確保您仍然符合向您的客戶保證的任何 SLA。

## 附錄：查詢和彙總效能測試

本附錄說明針對 Elasticsearch 叢集執行的效能測試。使用在一組個別的 VM 上執行的 JMeter 來執行測試。測試環境的組態詳細資料在〈如何建立 Elasticsearch 的效能測試環境〉文件中說明。若要執行自己的測試，您可以遵循本附錄中的指引手動建立自己的 JMeter 測試計劃，或使用個別可用的自動化測試指令碼。請參閱＜作法：執行自動化 Elasticsearch 查詢測試＞文件以取得詳細資訊。

資料查詢工作負載執行了如下所述的查詢集合，同時執行大規模的文件上傳 (資料已使用 JUnit 測試上傳，並遵循文件[利用 Azure 上的 Elasticsearch 最佳化資料擷取效能](https://github.com/mspnp/azure-guidance/blob/master/Elasticsearch-Data-Ingestion-Performance.md)所述的相同方法，進行資料擷取測試。) 此工作負載的目的是要模擬生產環境，新的資料會在執行搜尋時不斷地加入。查詢已結構化，只會擷取在過去 15 分鐘內加入之文件中的最新資料。

每份文件都儲存在名為 *idx* 的單一索引，而且具有 *doc* 類型。您可以使用下列 HTTP 要求來建立索引。*Number\_of\_replicas* 和 *number\_of\_shards* 設定和以下所示的許多測試中的值不同。此外，對於使用 fielddata 而不是文件值的測試，每個屬性會使用屬性 *"doc\_values": false* 加註。

> **重要事項**。索引已卸除並在執行每個測試之前重新建立。

``` http
PUT /idx
{  
    "settings" : {
        "number_of_replicas": 1,
        "refresh_interval": "30s",
        "number_of_shards": "5",
        "index.translog.durability": "async"    
    },
    "doc": {
        "mappings": {
            "event": {
                "_all": {
                    "enabled": false
                },
                "_timestamp": {
                    "enabled": true,
                    "store": true,
                    "format": "date_time"
                },
                "properties": {
                    "Organization": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "CustomField1": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "CustomField2": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "CustomField3": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "CustomField4": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "CustomField5": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "DateTimeReceivedUtc": {
                        "type": "date",
                        "format": "dateOptionalTime"
                    },
                    "Host": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "HttpMethod": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "HttpReferrer": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "HttpRequest": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "HttpUserAgent": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "HttpVersion": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "OrganizationName": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "SourceIp": {
                        "type": "ip"
                    },
                    "SourceIpAreaCode": {
                        "type": "long"
                    },
                    "SourceIpAsnNr": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "SourceIpBase10": {
                        "type": "long"
                    },
                    "SourceIpCity": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "SourceIpCountryCode": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "SourceIpLatitude": {
                        "type": "double"
                    },
                    "SourceIpLongitude": {
                        "type": "double"
                    },
                    "SourceIpMetroCode": {
                        "type": "long"
                    },
                    "SourceIpPostalCode": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "SourceIpRegion": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "SourceLatLong": {
                        "type": "geo_point",
                        "doc_values": true,
                        "lat_lon": true,
                        "geohash": true
                    },
                    "SourcePort": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "SourcedFrom": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "TargetIp": {
                        "type": "ip"
                    },
                    "TargetPort": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "Rating": {
                        "type": "string",
                        "index": "not_analyzed"
                    },
                    "UseHumanReadableDateTimes": {
                        "type": "boolean"
                    }
                }
            }
        }
    }
}
```

測試執行了下列查詢：
* 過去 15 分鐘內輸入多少個有 Rating 值的文件？

  ```http
  GET /idx/doc/_search
  {
    "query": {
      "bool": {
        "must": [
          {
            "range": {
              "DateTimeReceivedUtc": {
                "gte": "now-15m",
                "lte": "now"
              }
            }
          }
        ],
        "must_not": [],
        "should": []
      }
    },
    "from": 0,
    "size": 0,
    "aggs": {
      "2": {
        "terms": {
          "field": "Rating",
          "size": 5,
          "order": {
            "_count": "desc"
          }
        }
      }
    }
  }
  ```

* 過去 15 分鐘內，每 5 分鐘的間隔中加入多少個文件？

  ```http
  GET /idx/doc/_search
  {
    "query": {
      "bool": {
        "must": [
          {
            "range": {
              "DateTimeReceivedUtc": {
                "gte": "now-15m",
                "lte": "now"
              }
            }
          }
        ],
        "must_not": [],
        "should": []
      }
    },
    "from": 0,
    "size": 0,
    "sort": [],
    "aggs": {
      "2": {
        "date_histogram": {
          "field": "DateTimeReceivedUtc",
          "interval": "5m",
          "time_zone": "America/Los_Angeles",
          "min_doc_count": 1,
          "extended_bounds": {
            "min": "now-15m",
            "max": "now"
          }
        }
      }
    }
  }
  ```

* 過去 15 分鐘內，每個國家/地區的每個 Rating 值加入多少文件？

  ```HTTP
  GET /idx/doc/_search
  {
    "query": {
      "filtered": {
        "query": {
          "query_string": {
            "query": "*",
            "analyze_wildcard": true
          }
        },
        "filter": {
          "bool": {
            "must": [
              {
                "query": {
                  "query_string": {
                    "query": "*",
                    "analyze_wildcard": true
                  }
                }
              },
              {
                "range": {
                  "DateTimeReceivedUtc": {
                    "gte": "now-15m",
                    "lte": "now"
                  }
                }
              }
            ],
            "must_not": []
          }
        }
      }
    },
    "size": 0,
    "aggs": {
      "2": {
        "terms": {
          "field": "Rating",
          "size": 5,
          "order": {
            "_count": "desc"
          }
        },
        "aggs": {
          "3": {
            "terms": {
              "field": "SourceIpCountryCode",
              "size": 15,
              "order": {
                "_count": "desc"
              }
            }
          }
        }
      }
    }
  }
  ```

* 過去 15 分鐘內，哪 15 個組織最常有文件加入？

  ```http
  GET /idx/doc/_search
  {
    "query": {
      "filtered": {
        "query": {
          "query_string": {
            "query": "*",
            "analyze_wildcard": true
          }
        },
        "filter": {
          "bool": {
            "must": [
              {
                "query": {
                  "query_string": {
                    "query": "*",
                    "analyze_wildcard": true
                  }
                }
              },
              {
                "range": {
                  "DateTimeReceivedUtc": {
                    "gte": "now-15m",
                    "lte": "now"
                  }
                }
              }
            ],
            "must_not": []
          }
        }
      }
    },
    "size": 0,
    "aggs": {
      "2": {
        "terms": {
          "field": "Organization",
          "size": 15,
          "order": {
            "_count": "desc"
          }
        }
      }
    }
  }
  ```

* 過去 15 分鐘內，多少不同組織有文件加入？

  ```http
  GET /idx/doc/_search
  {
    "query": {
      "filtered": {
        "query": {
          "query_string": {
            "query": "*",
            "analyze_wildcard": true
          }
        },
        "filter": {
          "bool": {
            "must": [
              {
                "query": {
                  "query_string": {
                    "query": "*",
                    "analyze_wildcard": true
                  }
                }
              },
              {
                "range": {
                  "DateTimeReceivedUtc": {
                    "gte": "now-15m",
                    "lte": "now"
                  }
                }
              }
            ],
            "must_not": []
          }
        }
      }
    },
    "size": 0,
    "aggs": {
      "2": {
        "cardinality": {
          "field": "Organization"
        }
      }
    }
  }
  ```

* 過去 15 分鐘加入多少個文件？

  ```http
  GET /idx/doc/_search
  {
    "query": {
      "filtered": {
        "query": {
          "query_string": {
            "query": "*",
            "analyze_wildcard": true
          }
        },
        "filter": {
          "bool": {
            "must": [
              {
                "query": {
                  "query_string": {
                    "analyze_wildcard": true,
                    "query": "*"
                  }
                }
              },
              {
                "range": {
                  "DateTimeReceivedUtc": {
                    "gte": "now-15m",
                    "lte": "now"
                  }
                }
              }
            ],
            "must_not": []
          }
        }
      }
    },
    "size": 0,
    "aggs": {}
  }
  ```

* 過去 15 分鐘內，多少不同 SourceIp 有文件加入？

  ```http
  GET /idx/doc/_search
  {
    "query": {
      "filtered": {
        "query": {
          "query_string": {
            "query": "*",
            "analyze_wildcard": true
          }
        },
        "filter": {
          "bool": {
            "must": [
              {
                "query": {
                  "query_string": {
                    "query": "*",
                    "analyze_wildcard": true
                  }
                }
              },
              {
                "range": {
                  "DateTimeReceivedUtc": {
                    "gte": "now-15m",
                    "lte": "now"
                  }
                }
              }
            ],
            "must_not": []
          }
        }
      }
    },
    "size": 0,
    "aggs": {
      "2": {
        "cardinality": {
          "field": "SourceIp"
        }
      }
    }
  }
  ```

<!---HONumber=AcomDC_0302_2016-->